x-logging: &default-logging  # Defines a reusable logging configuration using YAML anchors
  options:
    max-size: "100m"  # Log rotation: max log size before rotation
    max-file: "5"  # Max number of log files to retain
  driver: json-file  # Specifies the logging driver, here using the default JSON file driver

x-deploy: &default-deploy  # Defines a reusable logging configuration using YAML anchors
  resources:
    reservations:
      devices:
        - driver: ${OLLAMA_GPU_DRIVER-nvidia}
          count: ${OLLAMA_GPU_COUNT-1}
          capabilities:
            - gpu

services:  # Defines the services part of the compose file
  host:  # Service definition for the main application host
    image: registry.aiempowerlabs.com/aistudio:latest  # Specifies the Docker image to use
    logging: *default-logging  # Uses the defined reusable logging configuration
    restart: unless-stopped  # Restart policy: restart if the container stops unexpectedly
    ports:
      - "8080:8080"  # Port mapping: Maps port 8080 of the container to port 8080 on the host
    environment:
      configPath: /opt/aistudio/  # Environment variable specifying the config path inside the container
      ASPNETCORE_URLS: http://*:8080
    healthcheck:  # Health check using a curl command to a local endpoint
      test: ["CMD", "curl", "-f", "http://localhost:8080/liveness"]
      interval: 1m
      timeout: 20s
      retries: 5
    volumes:
      - ./appsettings.json:/opt/aistudio/appsettings.json  # Mounts a configuration file into the container
    networks:
      - ai_internal  # Connects the service to the internal network
      - ai_public  # Connects the service to the public network
    depends_on:  # Specifies dependencies on other services
      - postgres
      - embedding
      - llama3
      - redis

  redis:  # Redis database service implemented in ValKey
    image: docker.io/valkey/valkey:latest  # Image to use for Redis / ValKey
    logging: *default-logging
    restart: unless-stopped
    networks:
      - ai_internal  # Connects the database service to the internal network only

  postgres:  # PostgreSQL database service
    image: paradedb/paradedb:latest  # Image to use for PostgreSQL with vector extension
    logging: *default-logging
    restart: unless-stopped
    environment:  # Database credentials
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    healthcheck:  # Checks the health of the service
      test: ["CMD-SHELL", "sh -c 'pg_isready -U postgres -d postgres'"]
      interval: 10s  # Health check interval
      timeout: 3s  # Timeout for the health check
      retries: 3  # Number of retries before considering the service unhealthy
    networks:
      - ai_internal  # Connects the database service to the internal network only

  llama3:  # Service for LLM llama3
    image: ollama/ollama
    logging: *default-logging
    restart: unless-stopped
    tty: true
    entrypoint: ["/bin/sh", "-c", "ollama serve & ollama pull llama3.1 && tail -f /dev/null"]
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    healthcheck:  # Health check using a curl command to a local endpoint
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 1m
      timeout: 20s
      retries: 5
    # Uncomment below to run on GPU support
    #deploy: &default-deploy
    volumes:
      - ./ollama-local:/root/.ollama
    networks:
      - ai_internal  # Connects to the internal network

  embedding:  # Embedding service
    image: registry.aiempowerlabs.com/embedding_base:latest  # Image to use for embedding service
    logging: *default-logging
    restart: unless-stopped
    healthcheck:  # Health check using a curl command to a local endpoint
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 1m  # Health check interval
      timeout: 10s  # Timeout for the health check
      retries: 3  # Number of retries before considering the service unhealthy
    # Uncomment below to run on GPU support
    #deploy: &default-deploy
    networks:
      - ai_internal  # Connects to the internal network

networks:
  ai_public: {}  # Definition of the public network
  ai_internal: {}  # Definition of the internal network
